{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "<div style=\"text-align: center;\"> <h1>GROUP F MACHINE LEARNING FOR DATA SCIENCE FINAL PROJECT</h1><> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook will be the main program from our project. When the project is finished, we will be able to use this Jupyter code to generate the presentation slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Scripts #Import all the scripts in the \"Scripts\" folder\n",
    "import pandas as pd\n",
    "import torch \n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import math\n",
    "import sys\n",
    "import sklearn\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to make is to import our training data and store it into a pandas structure and start the data preprocessing. Then we visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#Save the output value\u001b[39;00m\n\u001b[0;32m      4\u001b[0m OutputTraining \u001b[38;5;241m=\u001b[39m dataT[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLAID_UP_TIME\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 7\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfigsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m sns\u001b[38;5;241m.\u001b[39mhistplot(dataT[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLAID_UP_TIME\u001b[39m\u001b[38;5;124m'\u001b[39m], kde\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistribution of LAID_UP_TIME\u001b[39m\u001b[38;5;124m\"\u001b[39m, fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dataPath = \"./Data/TRAIN_Vehicles_Data.xlsx\" #The route to the data\n",
    "dataT = pd.read_excel(dataPath, na_values=[\"N\", \" \",\"Other\",\"Nicht definiert\"]) #After seeing the excel we set the null values as \"N\" and \" \"\n",
    "#Save the output value\n",
    "OutputTraining = dataT['LAID_UP_TIME']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(dataT['LAID_UP_TIME'], kde=True, bins=30, color='blue')\n",
    "plt.title(\"Distribution of LAID_UP_TIME\", fontsize=16)\n",
    "plt.xlabel(\"LAID_UP_TIME (days)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(dataT['LAID_UP_TIME'], color='orange')\n",
    "plt.title(\"Boxplot of LAID_UP_TIME\", fontsize=16)\n",
    "plt.xlabel(\"LAID_UP_TIME (days)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1) By inspection, remove all the columns that we stimate that will be not useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataT \u001b[38;5;241m=\u001b[39m \u001b[43mdataT\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRPAKREP_VEHICLE_HKEY\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCHASSIS_NUMBER\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOFFICE_MAIN_BRAND\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMANUFACTURER\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      2\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVEHICLE_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVARIANT\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRIM_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUPHOLSTERY_CODE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mENGINE_ID\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTRANSMISSION_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCOMMISSION_NUMBER\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLEASING_START\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLEASING_END\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLEASING_MILAGE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFINANCING_TYPE_NAME\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPURCHASE_DATE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPRICE_LIST\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSOLD_INVOICE_COSTUMER_ID\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSOLD_CUSTOMER_ID\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAID_UP_TIME\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataT' is not defined"
     ]
    }
   ],
   "source": [
    "dataT = dataT.drop(columns=[\"RPAKREP_VEHICLE_HKEY\",\"CHASSIS_NUMBER\",\"OFFICE_MAIN_BRAND\",\"MANUFACTURER\",\n",
    "                            \"VEHICLE_TYPE\",\"VARIANT\",\"RIM_KEY\",\"UPHOLSTERY_CODE\",\"ENGINE_ID\",\"TRANSMISSION_TYPE\",\n",
    "                            \"COMMISSION_NUMBER\",\"LEASING_START\",\"LEASING_END\",\"LEASING_MILAGE\",\"FINANCING_TYPE_NAME\",\n",
    "                            \"PURCHASE_DATE\",\"PRICE_LIST\",\"SOLD_INVOICE_COSTUMER_ID\",\"SOLD_CUSTOMER_ID\",\"LAID_UP_TIME\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2) Eliminate the columns with more 10% of its values being null. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after dropping columns with more than 10% null values:\n",
      "       COMPANY OFFICE MANUFACTURER_SHORT VEHICLE_GROUP  MILEAGE  \\\n",
      "0         44.0     TA                FOR           ECS   8600.0   \n",
      "1         12.0     44                VOL           XC4      0.0   \n",
      "2         27.0     33                FOR           TOC   1297.0   \n",
      "3         33.0     K1                  V           POL   6020.0   \n",
      "4         10.0     96                FOR           FOC      0.0   \n",
      "...        ...    ...                ...           ...      ...   \n",
      "99066     70.0      4                JAG           EPA     20.0   \n",
      "99067     17.0     60                FOR           FIE  67748.0   \n",
      "99068     33.0     KA                  V           AMA   3549.0   \n",
      "99069     10.0     10                FOR           TNT  14117.0   \n",
      "99070     17.0     61                ALF           STE  45013.0   \n",
      "\n",
      "       OPERATING_HOURS  MILAGE_IN_FIELD  MILAGE_SALES  OPERATING_HOURS_SALES  \\\n",
      "0                  0.0              1.0        8600.0                    0.0   \n",
      "1                  0.0              1.0           0.0                    0.0   \n",
      "2                  0.0              1.0           0.0                    0.0   \n",
      "3                  0.0              1.0        6020.0                    0.0   \n",
      "4                  0.0              1.0           0.0                    0.0   \n",
      "...                ...              ...           ...                    ...   \n",
      "99066              0.0              1.0          20.0                    0.0   \n",
      "99067              0.0              1.0       67748.0                    0.0   \n",
      "99068              0.0              1.0           6.0                    0.0   \n",
      "99069              0.0              1.0           0.0                    0.0   \n",
      "99070              0.0              1.0       45013.0                    0.0   \n",
      "\n",
      "       COLOR_CODE_NAME  ... OPERATION_HOURS_SALE SOLD_INVOICE_COSTUMER_ID2  \\\n",
      "0      Nicht definiert  ...                  0.0                    C38451   \n",
      "1      Nicht definiert  ...                  0.0                     40963   \n",
      "2      Nicht definiert  ...                  0.0                    496814   \n",
      "3      Nicht definiert  ...                  0.0                        50   \n",
      "4      Nicht definiert  ...                  0.0                     40963   \n",
      "...                ...  ...                  ...                       ...   \n",
      "99066  Nicht definiert  ...                  0.0                      1070   \n",
      "99067  Nicht definiert  ...                  0.0                     72428   \n",
      "99068  Nicht definiert  ...                  0.0                    483621   \n",
      "99069  Nicht definiert  ...                  0.0                    298739   \n",
      "99070  Nicht definiert  ...                  0.0                    D17445   \n",
      "\n",
      "       SALE_CUSTOMER_ID2 SCALED_CURRENT_VALUE  SCALED_INVENTURAL_VALUE  \\\n",
      "0                 C38451             0.052907                 0.000000   \n",
      "1                 493187             0.061605                 0.000000   \n",
      "2                 553344             0.063286                 0.000000   \n",
      "3                     50             0.042565                 0.029537   \n",
      "4                 A78589             0.054914                 0.000000   \n",
      "...                  ...                  ...                      ...   \n",
      "99066               1070             0.090608                 0.073237   \n",
      "99067              72428             0.048077                 0.000000   \n",
      "99068             483621             0.078745                 0.000000   \n",
      "99069             A81365             0.062609                 0.000000   \n",
      "99070             D17445             0.067036                 0.000000   \n",
      "\n",
      "       SCALED_REPORT_VALUE  SCALED_VALUATION_PRICE  SCALED_GUIDE_PRICE  \\\n",
      "0                 0.950043                     0.0            0.002975   \n",
      "1                 0.950043                     0.0            0.006136   \n",
      "2                 0.950043                     0.0            0.001378   \n",
      "3                 0.897257                     0.0            0.001378   \n",
      "4                 0.950043                     0.0            0.004863   \n",
      "...                    ...                     ...                 ...   \n",
      "99066             0.950043                     0.0            0.005363   \n",
      "99067             0.950043                     0.0            0.002620   \n",
      "99068             0.950043                     0.0            0.007444   \n",
      "99069             0.950043                     0.0            0.008290   \n",
      "99070             0.950043                     0.0            0.005367   \n",
      "\n",
      "       SCALED_TOTAL_SALES_PRICE_BASIS  SCALED_TOTAL_SALE_PRICE  \n",
      "0                            0.011018                 0.011321  \n",
      "1                            0.026411                 0.019844  \n",
      "2                            0.022102                 0.022053  \n",
      "3                            0.000000                 0.000572  \n",
      "4                            0.019715                 0.013360  \n",
      "...                               ...                      ...  \n",
      "99066                        0.025375                 0.024193  \n",
      "99067                        0.008127                 0.008242  \n",
      "99068                        0.040285                 0.037192  \n",
      "99069                        0.030804                 0.022721  \n",
      "99070                        0.026617                 0.025691  \n",
      "\n",
      "[99071 rows x 51 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "null_percentage = dataT.isnull().mean() * 100 #Multiply the mean by 100 to pass to percentage\n",
    "threshold = 10 #10% of null entries\n",
    "columns_to_drop = null_percentage[null_percentage > threshold].index #Extract the index of the columns with more of 10% of its values being null\n",
    "dataT = dataT.drop(columns=columns_to_drop) #Drop the columns\n",
    "print(\"\\nDataFrame after dropping columns with more than 10% null values:\")\n",
    "print(dataT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3) Discard caterogic columns that increases the dimensionality in excess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns to drop (more than 15 categories ): ['OFFICE', 'MANUFACTURER_SHORT', 'UPHOLSTERY', 'TRANSMISSION_NAME', 'FUEL_TYPE_NAME', 'SOLD_INVOICE_COSTUMER_ID2', 'SALE_CUSTOMER_ID2']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "categorical_cols = dataT.select_dtypes(include=['object', 'category']).columns\n",
    "num_one_hot_encoding = dataT[categorical_cols].nunique()\n",
    "excluded_cols = ['COLOR','VEHICLE_GROUP']\n",
    "cols_to_remove = num_one_hot_encoding[(num_one_hot_encoding > 10) & \n",
    "         (~num_one_hot_encoding.index.isin(excluded_cols)) ].index.tolist()\n",
    "print(f\"Columns to drop (more than 15 categories ): {cols_to_remove}\")\n",
    "data_categoric_removed = dataT.drop(columns=cols_to_remove)\n",
    "# Apply One Hot Encoding\n",
    "data_encoded = pd.get_dummies(data_categoric_removed, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'date_column'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\jonan\\Documents\\BIP_Project\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'date_column'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m OutputTraining_clean \u001b[38;5;241m=\u001b[39m temp_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 2. Supongamos que \"date_column\" es de tipo datetime64; creamos nuevas columnas numéricas\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m data_encoded_clean[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata_encoded_clean\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdate_column\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39myear\n\u001b[0;32m     11\u001b[0m data_encoded_clean[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m data_encoded_clean[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate_column\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mmonth\n\u001b[0;32m     12\u001b[0m data_encoded_clean[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mday\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m data_encoded_clean[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate_column\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mday\n",
      "File \u001b[1;32mc:\\Users\\jonan\\Documents\\BIP_Project\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\jonan\\Documents\\BIP_Project\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'date_column'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "temp_df = data_encoded.copy()\n",
    "temp_df[\"target\"] = OutputTraining  \n",
    "temp_df.dropna(inplace=True)\n",
    "\n",
    "data_encoded_clean = temp_df.drop(columns=[\"target\"])\n",
    "OutputTraining_clean = temp_df[\"target\"]\n",
    "\n",
    "mi_scores = mutual_info_regression(\n",
    "    data_encoded_clean,\n",
    "    OutputTraining_clean,\n",
    "    discrete_features=False\n",
    ")\n",
    "\n",
    "columns = data_encoded_clean.columns\n",
    "threshold = 0.4\n",
    "\n",
    "columns_to_drop = columns[mi_scores < threshold].tolist()\n",
    "print(\"Columns to drop (MI < 0.4):\", columns_to_drop)\n",
    "\n",
    "data_cleaned = data_encoded_clean.drop(columns=columns_to_drop)\n",
    "\n",
    "    \n",
    "print(f\"\\nColumns to drop (Mutual Information < 0.4): {columns_to_drop}\")\n",
    "data_cleaned = data_encoded_clean.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3) Statistic description of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After preprocesing we have: 13 columns left \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SCALED_INVENTURAL_VALUE</th>\n",
       "      <th>SCALED_REPORT_VALUE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>99004.000000</td>\n",
       "      <td>99004.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.013350</td>\n",
       "      <td>0.944832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.031157</td>\n",
       "      <td>0.030980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.950043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.950043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.950043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.871569</td>\n",
       "      <td>0.963855</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       SCALED_INVENTURAL_VALUE  SCALED_REPORT_VALUE\n",
       "count             99004.000000         99004.000000\n",
       "mean                  0.013350             0.944832\n",
       "std                   0.031157             0.030980\n",
       "min                   0.000000             0.000000\n",
       "25%                   0.000000             0.950043\n",
       "50%                   0.000000             0.950043\n",
       "75%                   0.000000             0.950043\n",
       "max                   0.871569             0.963855"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_columns = len(data_cleaned.columns)\n",
    "print(f\"After preprocesing we have: {n_columns} columns left \")\n",
    "pd.set_option('display.max_columns', None) #To see all the columns\n",
    "data_cleaned.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4) Visualization of the variable type of the attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_selection\u001b[39;00m\n\u001b[0;32m      4\u001b[0m data_cleaned\u001b[38;5;241m.\u001b[39mdtypes\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m data_cleaned\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "\n",
    "data_cleaned.dtypes\n",
    "\n",
    "for column in data_cleaned.columns:\n",
    "    \n",
    "    correlation = mutual_info_regression(data_cleaned[column],OutputTraining)\n",
    "    \n",
    "    \n",
    "    print(f\"Correlation between '{column}' y 'OutputTraining': {correlation:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
